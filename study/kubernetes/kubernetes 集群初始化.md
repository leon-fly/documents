

# kubernetesÂü∫‰∫éubuntuÈõÜÁæ§ÂàùÂßãÂåñ 

## 1. ÁéØÂ¢É

‰ΩøÁî®ÁéØÂ¢É‰∏∫ËÖæËÆØ‰∫ëÔºåÂÖ∑‰ΩìÁ°¨‰ª∂‰ø°ÊÅØÂ¶Ç‰∏ãÔºö

```
lighthouse@VM-4-11-ubuntu:~$ uname -a
Linux VM-4-11-ubuntu 5.4.0-153-generic #170-Ubuntu SMP Fri Jun 16 13:43:31 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

lighthouse@VM-4-11-ubuntu:~$ cat /proc/version
Linux version 5.4.0-153-generic (buildd@bos03-amd64-008) (gcc version 9.4.0 (Ubuntu 9.4.0-1ubuntu1~20.04.1)) #170-Ubuntu SMP Fri Jun 16 13:43:31 UTC 2023

lighthouse@VM-4-11-ubuntu:~$ cat /proc/cpuinfo | grep "cpu cores" | uniq
cpu cores       : 2

lighthouse@VM-4-11-ubuntu:~$ free -h
              total        used        free      shared  buff/cache   available
Mem:          3.3Gi       438Mi       174Mi       2.0Mi       2.7Gi       2.6Gi
Swap:            0B          0B          0B
```

## 2. ÂÆâË£Ökubeadm

[kubeadm](https://github.com/kubernetes/kubeadm)ÊòØ‰∏Ä‰∏™2017Âπ¥ÂèëËµ∑ÁöÑ‰∏Ä‰∏™kubernetesÈõÜÁæ§ÈÉ®ÁΩ≤ÁÆ°ÁêÜÂ∑•ÂÖ∑Ôºå ÂèØ‰ª•‰æøÊç∑ÁöÑÂØπËøõË°åÈõÜÁæ§ÁÆ°ÁêÜ„ÄÇ

> sudo apt-get install kubeadm

```
lighthouse@VM-4-11-ubuntu:~$ kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"28", GitVersion:"v1.28.2", GitCommit:"89a4ea3e1e4ddd7f7572286090359983e0387b2f", GitTreeState:"clean", BuildDate:"2023-09-13T09:34:32Z", GoVersion:"go1.20.8", Compiler:"gc", Platform:"linux/amd64"}
```

## 3. ÂÆâË£Ödocker

> apt-get install -y docker.io

## 4. ÂÆâË£Öcri-dockerd

Ëé∑ÂèñÂØπÂ∫îÁ≥ªÁªüÁöÑcri-dockered, ÂΩìÂâçÁ≥ªÁªü‰ΩøÁî®ÁöÑÊòØÂ¶Ç‰∏ãÁâàÊú¨

> wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.9/cri-dockerd_0.3.9.3-0.ubuntu-focal_amd64.deb

ÂÆâË£Ö

> sudo dpkg -i cri-dockerd_0.3.9.3-0.ubuntu-focal_amd64.deb

‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂`/lib/systemd/system/cri-docker.service` Â¢ûÂä†Â¶Ç‰∏ãÈÖçÁΩÆ

```
ExecStart=/usr/bin/cri-dockerd --network-plugin=cni  --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9
```

ÂêØÂä®

> systemctl start cri-docker

Êü•ÁúãÂêØÂä®ÊÉÖÂÜµ

> systemctl status cri-docker.service

```
lighthouse@VM-4-11-ubuntu:/etc$ sudo systemctl status cri-docker.service
‚óè cri-docker.service - CRI Interface for Docker Application Container Engine
     Loaded: loaded (/lib/systemd/system/cri-docker.service; enabled; vendor preset: enabled)
     Active: active (running) since Thu 2024-02-08 00:35:04 CST; 4s ago
TriggeredBy: ‚óè cri-docker.socket
       Docs: https://docs.mirantis.com
   Main PID: 2025478 (cri-dockerd)
      Tasks: 8
     Memory: 31.4M
     CGroup: /system.slice/cri-docker.service
             ‚îî‚îÄ2025478 /usr/bin/cri-dockerd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_c>

Feb 08 00:35:04 VM-4-11-ubuntu cri-dockerd[2025478]: time="2024-02-08T00:35:04+08:00" level=info msg="Connecting to docker >
Feb 08 00:35:04 VM-4-11-ubuntu cri-dockerd[2025478]: time="2024-02-08T00:35:04+08:00" level=info msg="Start docker client w>
Feb 08 00:35:04 VM-4-11-ubuntu cri-dockerd[2025478]: time="2024-02-08T00:35:04+08:00" level=info msg="Hairpin mode is set t>
Feb 08 00:35:04 VM-4-11-ubuntu cri-dockerd[2025478]: time="2024-02-08T00:35:04+08:00" level=info msg="Loaded network plugin>
Feb 08 00:35:04 VM-4-11-ubuntu cri-dockerd[2025478]: time="2024-02-08T00:35:04+08:00" level=info msg="Docker cri networking>
Feb 08 00:35:04 VM-4-11-ubuntu cri-dockerd[2025478]: time="2024-02-08T00:35:04+08:00" level=info msg="Setting cgroupDriver >
Feb 08 00:35:04 VM-4-11-ubuntu cri-dockerd[2025478]: time="2024-02-08T00:35:04+08:00" level=info msg="Docker cri received r>
Feb 08 00:35:04 VM-4-11-ubuntu cri-dockerd[2025478]: time="2024-02-08T00:35:04+08:00" level=info msg="Starting the GRPC bac>
Feb 08 00:35:04 VM-4-11-ubuntu cri-dockerd[2025478]: time="2024-02-08T00:35:04+08:00" level=info msg="Start cri-dockerd grp>
Feb 08 00:35:04 VM-4-11-ubuntu systemd[1]: Started CRI Interface for Docker Application Container Engine.
```

## 5. ÂàùÂßãÂåñkubernetesÈõÜÁæ§

### Init cluster

#### üëâ ÊâßË°åinit

**ÊñπÂºè‰∏ÄÔºàÊé®ËçêÔºâÔºö**

Â∞ÜÂàùÂßãÂåñÂèÇÊï∞ÈõÜ‰∏≠ÂåñÂà∞ÈÖçÁΩÆÊñá‰ª∂ÔºåÂú®ÂêØÂä®Êó∂ÊåáÂÆöÈÖçÁΩÆÊñá‰ª∂`kubeadm init --config kubeadm.yaml`‰æø‰∫éÁª¥Êä§ÁÆ°ÁêÜÔºåÈÖçÁΩÆÊñá‰ª∂kubeadm.yaml ÂèÇËÄÉ[ÂÆòÊñπkubeadmÈÖçÁΩÆ](https://kubernetes.io/zh-cn/docs/reference/config-api/kubeadm-config.v1beta3/)

**ÊñπÂºè‰∫åÔºàÊú¨ÊñáÊ°£ÂÆûÈôÖÊâßË°åÔºâÔºö**

> sudo kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers --cri-socket unix:///var/run/cri-dockerd.sock --pod-network-cidr=10.244.0.0/16

```
lighthouse@VM-4-11-ubuntu:~/my-kubernate-config$ sudo kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers --cri-socket unix:///var/run/cri-dockerd.sock --pod-network-cidr=10.244.0.0/16
I0208 01:36:47.117631 2043585 version.go:256] remote version is much newer: v1.29.1; falling back to: stable-1.28
[init] Using Kubernetes version: v1.28.6
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0208 01:36:48.411217 2043585 checks.go:835] detected that the sandbox image "registry.aliyuncs.com/google_containers/pause:3.9" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local vm-4-11-ubuntu] and IPs [10.96.0.1 10.0.4.11]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost vm-4-11-ubuntu] and IPs [10.0.4.11 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost vm-4-11-ubuntu] and IPs [10.0.4.11 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 6.502537 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node vm-4-11-ubuntu as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node vm-4-11-ubuntu as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: 9447k1.usqndnxye4mbaugw
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.0.4.11:6443 --token 9447k1.usqndnxye4mbaugw \
        --discovery-token-ca-cert-hash sha256:b8f805dd4bc6c7064512a8331fc6b8cf64dd7a7479d3f0cd81b327c56431304f 
```

Êé•‰∏ãÊù•ÈúÄË¶ÅÊåâÁÖßÊó•ÂøóÊèêÁ§∫ÂêéÁª≠Âá†Ê≠•Êìç‰Ωú

#### üëâ Êã∑Ë¥ùÈÖçÁΩÆÊñá‰ª∂

>mkdir -p $HOME/.kube
>
>sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
>
>sudo chown $(id -u):$(id -g) $HOME/.kube/config

Âê¶Âàô‰ºöÂá∫Áé∞Â¶Ç‰∏ãÈóÆÈ¢òÔºö

```
lighthouse@VM-4-11-ubuntu:~$ kubectl get no -A
E0203 21:37:19.797023 1319741 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0203 21:37:19.797382 1319741 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0203 21:37:19.799397 1319741 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0203 21:37:19.800736 1319741 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0203 21:37:19.802018 1319741 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

Ê≠§Êó∂Êü•ÁúãÂèØËÉΩÁúãÂà∞corednsÁä∂ÊÄÅÊó∂pending

```
lighthouse@VM-4-11-ubuntu:~$ kubectl get no -A
NAME             STATUS     ROLES           AGE     VERSION
vm-4-11-ubuntu   NotReady   control-plane   3h12m   v1.28.2

lighthouse@VM-4-11-ubuntu:~$ kubectl get pods --all-namespaces
NAMESPACE     NAME                                     READY   STATUS    RESTARTS   AGE
kube-system   coredns-6554b8b87f-52kvm                 0/1     Pending   0          3h6m
kube-system   coredns-6554b8b87f-k5fmr                 0/1     Pending   0          3h6m
kube-system   etcd-vm-4-11-ubuntu                      1/1     Running   0          3h6m
kube-system   kube-apiserver-vm-4-11-ubuntu            1/1     Running   0          3h6m
kube-system   kube-controller-manager-vm-4-11-ubuntu   1/1     Running   0          3h6m
kube-system   kube-proxy-s6q6m                         1/1     Running   0          3h6m
kube-system   kube-scheduler-vm-4-11-ubuntu            1/1     Running   0          3h6m
```

Êü•‰∏Ä‰∏ãÊó•ÂøóÂèëÁé∞ÁΩëÁªúÈÖçÁΩÆÊ≤°ÊúâÂàùÂßãÂåñ

```
sudo journalctl -f -u kubelet.service
-- Logs begin at Mon 2024-01-29 03:23:41 CST. --
Feb 03 22:07:02 VM-4-11-ubuntu kubelet[1275240]: E0203 22:07:02.115716 1275240 kubelet.go:2855] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized"
......
```

#### üëâ ÂàùÂßãÂåñÁΩëÁªúÊèí‰ª∂

 [ÂÆòÊñπÂàóÂá∫ÁöÑÁΩëÁªúÊèí‰ª∂Ê∏ÖÂçï](https://kubernetes.io/docs/concepts/cluster-administration/addons/)ËøôÈáåÊàë‰ª¨‰ΩøÁî®Â∏∏Áî®ÁöÑÂÖ∂‰∏≠‰∏Ä‰∏™[ÁΩëÁªúÊèí‰ª∂flannel](https://github.com/flannel-io/flannel#deploying-flannel-manually), ÊåâÁÖßËØ¥ÊòéreadmeËøõË°åÊèí‰ª∂ÂÆâË£Ö

```
lighthouse@VM-4-11-ubuntu:~$ kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml 
namespace/kube-flannel created
serviceaccount/flannel created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
```

üî•ÂΩìÂú®ÂàùÂßãÂåñÈõÜÁæ§Êó∂Â¶ÇÊûúÊ≤°ÊúâÊåáÂÆöÂèÇÊï∞`--pod-network-cidr=10.244.0.0/16` Êó∂‰ºöÂá∫Áé∞Â¶Ç‰∏ãÈóÆÈ¢òÔºöÊü•Áúãkube-flannelÂêØÂä®ÊÉÖÂÜµÔºåÂèëÁé∞flannelÁöÑÂèëÂ∏ÉÁä∂ÊÄÅÊòØCrashLoopBackOff (ÈáçÂêØÂá†Ê¨°‰πãÂêéÂèò‰∏∫ÊòØError)

```
lighthouse@VM-4-11-ubuntu:~$ kubectl get pods --all-namespaces
NAMESPACE      NAME                                     READY   STATUS              RESTARTS      AGE
kube-flannel   kube-flannel-ds-q4knd                    0/1     CrashLoopBackOff    2 (15s ago)   39s
kube-system    coredns-6554b8b87f-52kvm                 0/1     ContainerCreating   0             4h8m
kube-system    coredns-6554b8b87f-k5fmr                 0/1     ContainerCreating   0             4h8m
kube-system    etcd-vm-4-11-ubuntu                      1/1     Running             0             4h8m
kube-system    kube-apiserver-vm-4-11-ubuntu            1/1     Running             0             4h8m
kube-system    kube-controller-manager-vm-4-11-ubuntu   1/1     Running             0             4h8m
kube-system    kube-proxy-s6q6m                         1/1     Running             0             4h8m
kube-system    kube-scheduler-vm-4-11-ubuntu            1/1     Running             0             4h8m
```

Êü•ÁúãÊó•ÂøóÁ°ÆÂÆö[flannelÁöÑÂèëÂ∏ÉÁä∂ÊÄÅÊòØCrashLoopBackOffÂéüÂõ†](https://www.cnblogs.com/360linux/p/12933594.html) : ËäÇÁÇπ‰∏ãÁöÑpod cidrÊ≤°ÊúâÂàÜÈÖç

```
lighthouse@VM-4-11-ubuntu:~$ kubectl logs -f --tail 200 -n kube-flannel kube-flannel-ds-q4knd
Defaulted container "kube-flannel" out of: kube-flannel, install-cni-plugin (init), install-cni (init)
I0204 03:23:24.086258       1 main.go:209] CLI flags config: {etcdEndpoints:http://127.0.0.1:4001,http://127.0.0.1:2379 etcdPrefix:/coreos.com/network etcdKeyfile: etcdCertfile: etcdCAFile: etcdUsername: etcdPassword: version:false kubeSubnetMgr:true kubeApiUrl: kubeAnnotationPrefix:flannel.alpha.coreos.com kubeConfigFile: iface:[] ifaceRegex:[] ipMasq:true ifaceCanReach: subnetFile:/run/flannel/subnet.env publicIP: publicIPv6: subnetLeaseRenewMargin:60 healthzIP:0.0.0.0 healthzPort:0 iptablesResyncSeconds:5 iptablesForwardRules:true netConfPath:/etc/kube-flannel/net-conf.json setNodeNetworkUnavailable:true}
W0204 03:23:24.086339       1 client_config.go:617] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0204 03:23:24.099440       1 kube.go:139] Waiting 10m0s for node controller to sync
I0204 03:23:24.099491       1 kube.go:461] Starting kube subnet manager
I0204 03:23:25.099595       1 kube.go:146] Node controller sync successful
I0204 03:23:25.099622       1 main.go:229] Created subnet manager: Kubernetes Subnet Manager - vm-4-11-ubuntu
I0204 03:23:25.099627       1 main.go:232] Installing signal handlers
I0204 03:23:25.099750       1 main.go:540] Found network config - Backend type: vxlan
I0204 03:23:25.099777       1 match.go:210] Determining IP address of default interface
I0204 03:23:25.100059       1 match.go:263] Using interface with name eth0 and address 10.0.4.11
I0204 03:23:25.100084       1 match.go:285] Defaulting external address to interface address (10.0.4.11)
I0204 03:23:25.100138       1 vxlan.go:141] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
I0204 03:23:25.102693       1 kube.go:621] List of node(vm-4-11-ubuntu) annotations: map[string]string{"kubeadm.alpha.kubernetes.io/cri-socket":"unix:///var/run/cri-dockerd.sock", "node.alpha.kubernetes.io/ttl":"0", "volumes.kubernetes.io/controller-managed-attach-detach":"true"}
E0204 03:23:25.102983       1 main.go:332] Error registering network: failed to acquire lease: node "vm-4-11-ubuntu" pod cidr not assigned
W0204 03:23:25.103079       1 reflector.go:347] pkg/subnet/kube/kube.go:462: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
I0204 03:23:25.103099       1 main.go:520] Stopping shutdownHandler...
```

ÂØπÂ∫îËß£ÂÜ≥ÊñπÊ°à

* podÂ≠êÁΩëÁΩëÊÆµ‰øÆÊîπ

  > kubectl edit cm kubeadm-config -n kube-system

‚Äã       Âú®networking‰∏ãÊ∑ªÂä†ÈÖçÁΩÆ podSubnet: 10.244.0.0/16

* ‰øÆÊîπÈùôÊÄÅpod kube-controller-managerÈÖçÁΩÆ

  > vim /etc/kubernetes/manifests/kube-controller-manager.yaml 

  Ê∑ªÂä†ÂêØÂä®ÂèÇÊï∞

  ```
  - --allocate-node-cidrs=true 
  - --cluster-cidr=10.244.0.0/16
  ```

* ‰øÆÊîπÂÆåÊàê‰πãÂêéÈáçÂª∫ÂêØÂä®flannel pod

  ```
  kubectl delete -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
  kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
  ```

Ëøô‰∫õÂÆåÊàê‰πãÂêéÂÜçÁúãÂêÑ‰∏™podsÊÉÖÂÜµÔºåflannelÂíåcorednsÁöÑpodÈÉΩÂ§Ñ‰∫érunningÁä∂ÊÄÅ„ÄÇ

### ÈõÜÁæ§ËäÇÁÇπÂä†ÂÖ•

‰ΩøÁî®`kubeadm join`Âä†ÂÖ•workerËäÇÁÇπÔºåÊåá‰ª§ÂèÇÊï∞ÂèØ‰ª•‰ªéÂêØÂä®Êó•ÂøóËé∑Âèñ

> kubeadm join 10.0.4.11:6443 --token 9447k1.usqndnxye4mbaugw \
>         --discovery-token-ca-cert-hash sha256:b8f805dd4bc6c7064512a8331fc6b8cf64dd7a7479d3f0cd81b327c56431304f 

Êü•ÁúãËäÇÁÇπÂä†ÂÖ•ÊÉÖÂÜµ

> kubeadm get no -A

### [dashboard ÈÉ®ÁΩ≤](https://kubernetes.io/zh-cn/docs/tasks/access-application-cluster/web-ui-dashboard/)

#### üëâ ÈÉ®ÁΩ≤

> kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

ÂçïÊú∫ÁéØÂ¢É‰∏ãËäÇÁÇπ‰∏äËÆæÁΩÆ‰∫ÜÊ±°ÁÇπÂØºËá¥Ë∞ÉÂ∫¶Â§±Ë¥•(PendingÁä∂ÊÄÅ), Â§±Ë¥•ÈîôËØØ‰ø°ÊÅØÂèØ‰ª•ÈÄöËøáÊ±°ÁÇπÂÆπÂøçÈÖçÁΩÆËß£ÂÜ≥„ÄÇ

```
lighthouse@VM-4-11-ubuntu:~$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created

lighthouse@VM-4-11-ubuntu:~$ kubectl get pods --all-namespaces
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE
kube-flannel           kube-flannel-ds-hcj6v                        1/1     Running   0          66m
kube-system            coredns-6554b8b87f-52kvm                     1/1     Running   0          18h
kube-system            coredns-6554b8b87f-k5fmr                     1/1     Running   0          18h
kube-system            etcd-vm-4-11-ubuntu                          1/1     Running   0          18h
kube-system            kube-apiserver-vm-4-11-ubuntu                1/1     Running   0          18h
kube-system            kube-controller-manager-vm-4-11-ubuntu       1/1     Running   0          68m
kube-system            kube-proxy-s6q6m                             1/1     Running   0          18h
kube-system            kube-scheduler-vm-4-11-ubuntu                1/1     Running   0          18h
kubernetes-dashboard   dashboard-metrics-scraper-5657497c4c-bzp6g   0/1     Pending   0          39s
kubernetes-dashboard   kubernetes-dashboard-78f87ddfc-htfhs         0/1     Pending   0          39s
```

podË∞ÉÂ∫¶ÊÉÖÂÜµÊü•Áúã

```
lighthouse@VM-4-11-ubuntu:~$ kubectl describe pod dashboard-metrics-scraper-5657497c4c-bzp6g -n kubernetes-dashboardName:             dashboard-metrics-scraper-5657497c4c-bzp6g
Namespace:        kubernetes-dashboard
Priority:         0
Service Account:  kubernetes-dashboard
Node:             <none>
Labels:           k8s-app=dashboard-metrics-scraper
                  pod-template-hash=5657497c4c
Annotations:      <none>
Status:           Pending
SeccompProfile:   RuntimeDefault
IP:               
IPs:              <none>
Controlled By:    ReplicaSet/dashboard-metrics-scraper-5657497c4c
Containers:
  dashboard-metrics-scraper:
    Image:        kubernetesui/metrics-scraper:v1.0.8
    Port:         8000/TCP
    Host Port:    0/TCP
    Liveness:     http-get http://:8000/ delay=30s timeout=30s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-l7bt9 (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  tmp-volume:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-l7bt9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age    From               Message
  ----     ------            ----   ----               -------
  Warning  FailedScheduling  2m58s  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..
```

Êü•Áúãnode‰∏äËÆæÁΩÆÁöÑÊ±°ÁÇπ‰ø°ÊÅØÔºàÂèÇËÄÉ[Ê±°ÁÇπ‰∏éÂÆπÂøç](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)ÔºâÔºö

```
lighthouse@VM-4-11-ubuntu:~$ kubectl describe node vm-4-11-ubuntu|grep Taints
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
```

**Ëß£ÂÜ≥ÊñπÊ°àÔºö**

**ÊñπÊ°à‰∏ÄÔºö**ÁßªÈô§ËäÇÁÇπ‰∏äÁöÑÊ±°ÁÇπ`kubectl taint nodes <hostname> <traints>-` , Ê≠§Â§ÑÊâßË°åÔºö

```
lighthouse@VM-4-11-ubuntu:~$ kubectl taint nodes vm-4-11-ubuntu node-role.kubernetes.io/control-plane:NoSchedule-
node/vm-4-11-ubuntu untainted
```



**ÊñπÊ°à‰∫åÔºö**Â∞ÜdashboardÁöÑÈÉ®ÁΩ≤Êñá‰ª∂‰∏≠‰∏§‰∏™podÈÖçÁΩÆÊ±°ÁÇπÂÆπÂøç(spec.template.spec.tolerationsËäÇÁÇπ‰∏ãÂ¢ûÂä†Â¶Ç‰∏ãÈÖçÁΩÆÔºåÂÖ±‰∏§‰∏™Pod‰∏§Â§Ñ)Ôºö

```
- key: node-role.kubernetes.io/control-plane
          effect: NoSchedule
```

apply‰Ωø‰øÆÊîπÁîüÊïà

```
# ‰∏ãËΩΩÊñá‰ª∂‰øÆÊîπ‰øùÂ≠òÂà∞Êú¨Âú∞
lighthouse@VM-4-11-ubuntu:~$  kubectl apply -f ~/k8s-offline-configuration/my-dashboard-deployment.yamlnamespace/kubernetes-dashboard unchanged
serviceaccount/kubernetes-dashboard unchanged
service/kubernetes-dashboard unchanged
secret/kubernetes-dashboard-certs unchanged
secret/kubernetes-dashboard-csrf unchanged
secret/kubernetes-dashboard-key-holder unchanged
configmap/kubernetes-dashboard-settings unchanged
role.rbac.authorization.k8s.io/kubernetes-dashboard unchanged
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard unchanged
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard unchanged
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard unchanged
deployment.apps/kubernetes-dashboard configured
service/dashboard-metrics-scraper unchanged
deployment.apps/dashboard-metrics-scraper configured
```

ÂÜçÊü•ÁúãpodsÊÉÖÂÜµ, Â∑≤Â§Ñ‰∫érunningÁä∂ÊÄÅ

```
lighthouse@VM-4-11-ubuntu:~$ kubectl get pods --all-namespaces
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE
kube-flannel           kube-flannel-ds-hcj6v                        1/1     Running   0          81m
kube-system            coredns-6554b8b87f-52kvm                     1/1     Running   0          18h
kube-system            coredns-6554b8b87f-k5fmr                     1/1     Running   0          18h
kube-system            etcd-vm-4-11-ubuntu                          1/1     Running   0          18h
kube-system            kube-apiserver-vm-4-11-ubuntu                1/1     Running   0          18h
kube-system            kube-controller-manager-vm-4-11-ubuntu       1/1     Running   0          83m
kube-system            kube-proxy-s6q6m                             1/1     Running   0          18h
kube-system            kube-scheduler-vm-4-11-ubuntu                1/1     Running   0          18h
kubernetes-dashboard   dashboard-metrics-scraper-5bd9b4ff89-qtx48   1/1     Running   0          115s
kubernetes-dashboard   kubernetes-dashboard-658b94579f-qdd7v        1/1     Running   0          115s
```

#### üëâ  ‰ª£ÁêÜÁ´ØÂè£‰ΩøËØ•podÁöÑÊúçÂä°ËÉΩË¢´Â§ñÈÉ®ËÆøÈóÆ

> kubectl proxy --address='0.0.0.0'  --accept-hosts='^*$' --port=8001

```
lighthouse@VM-4-11-ubuntu:~$ kubectl proxy --address='0.0.0.0'  --accept-hosts='^*$' --port=8001
Starting to serve on [::]:8001
```

‰ª•‰∏äÊñπÂºèÂèØ‰ª•Êé•ÂèóÊâÄÊúâ‰∏ªÊú∫ËØ∑Ê±ÇÔºåÂÆòÊñπÁªôÂá∫ÁöÑÂëΩ‰ª§`kubuctl proxy` ‰∏çÂ∏¶‰ªª‰ΩïÂèÇÊï∞Âè™ËÉΩÂú®ÂÆø‰∏ªÊú∫‰∏ä‰ΩøÁî®„ÄÇË¶ÅËÉΩ‰ΩøÂ§ñÈÉ®ËÉΩÂ§ü‰ΩøÁî®ËøòÈúÄË¶ÅÊâìÂºÄÈò≤ÁÅ´Â¢ô„ÄÇÊú¨Âú∞‰øÆÊîπiptables:

> sudo iptables -I INPUT -p tcp --dport 8001 -j ACCEPT
>
> sudo iptables-save

ÂΩìÁÑ∂Â¶ÇÊûúÊòØ‰ªéÂ§ñÁΩëËÆøÈóÆËøòÈúÄË¶ÅÈÖçÁΩÆÂØπÂ∫î‰∫ëÂπ≥Âè∞ÁöÑÁΩëÁªúÁ≠ñÁï•ÂºÄÊîæÁõ∏Â∫îÁ´ØÂè£„ÄÇ

#### üëâ Êú¨Âú∞sshËΩ¨Âèë

dashboard‰ªÖ‰ªÖÂÖÅËÆ∏localhost / 127.0.0.1ÈÄöËøáhttpËÆøÈóÆÔºåÂ¶ÇÊûúÊòØÂÖ∂‰ªñÂú∞ÂùÄÈúÄË¶ÅËµ∞httpsÂçèËÆÆÔºåÊâÄ‰ª•Êàë‰ª¨ÈúÄË¶ÅÂú®Êú¨Âú∞ÈÄöËøásshËΩ¨ÂèëÂà∞ÂÆø‰∏ªÊú∫

> ssh -L localhost:8001:localhost:8001 -NT lighthouse@101.34.243.187
> lighthouse@101.34.243.187's password:

ÁÑ∂ÂêéÊú¨Âú∞Áõ¥Êé•ÈÄöËøálocalhostËÆøÈóÆ http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ 

Âè¶ÂèØÂèÇËÄÉ[ÂÆòÊñπÊèê‰æõÁöÑÂÖ∂‰ªñËÆøÈóÆdashboardÁöÑÊñπÂºè](https://github.com/kubernetes/dashboard/blob/master/docs/user/accessing-dashboard/README.md#login-not-available)

#### üëâ  [ÂàõÂª∫dashboardÁôªÂΩïÁî®Êà∑](https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md)

* ÁºñËæë`my-dashboard-user.yaml` Êñá‰ª∂ÂÜÖÂÆπ

  ```
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: admin-user
    namespace: kubernetes-dashboard
  
  ---
  
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: admin-user
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: cluster-admin
  subjects:
  - kind: ServiceAccount
    name: admin-user
    namespace: kubernetes-dashboard
  
  --- 
  apiVersion: v1
  kind: Secret
  metadata:
    name: admin-user
    namespace: kubernetes-dashboard
    annotations:
      kubernetes.io/service-account.name: "admin-user"   
  type: kubernetes.io/service-account-token  
  ```

  ËØ•Êñá‰ª∂ÂÆö‰πâ‰∫ÜÂá†‰∏™‰∫ãÊÉÖÔºö

  1. ÂàõÂª∫‰∏Ä‰∏™admin-userË¥¶Âè∑
  2. ÁªôËØ•Ë¥¶Âè∑ÁªëÂÆö‰∏Ä‰∏™ËßíËâ≤admin-user
  3. ‰∏∫Áî®Êà∑admin-userÂàõÂª∫‰∏Ä‰∏™<font color=red>long-lived token</font> ÔºàÂ¶ÇÊûúÂè™ÈúÄË¶Å‰∏Ä‰∏™Áü≠ÊúütokenÔºåËøôÈÉ®ÂàÜÂÆö‰πâÂèØ‰ª•‰∏çÁî®Ôºâ

* ÈÄöËøáapiËøõË°åËµÑÊ∫êÂàõÂª∫

  ```
  lighthouse@VM-4-11-ubuntu:~$ kubectl apply -f ~/k8s-offline-configuration/my-dashboard-user.yaml 
  serviceaccount/admin-user created
  clusterrolebinding.rbac.authorization.k8s.io/admin-user created
  secret/admin-user created
  ```

* Ëé∑Âèñlong-live token

  > kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath={".data.token"} | base64 -d

  üî• ÈÅøÂùëÔºö ‰ª•‰∏ä‰∏∫ÂÆòÊñπÊèê‰æõÁöÑÂëΩ‰ª§Ë°åÔºåËøêË°åÂÆå‰πãÂêéÂ±ïÁ§∫ÂÜÖÂÆπ‰ºö‰∏é‰∏ã‰∏Ä‰∏™ÂëΩ‰ª§Ëµ∑ÂßãËøûÂà∞‰∏ÄÂùóÔºåÊã∑Ë¥ùtokenÁöÑÊó∂ÂÄôÊ≥®ÊÑèÊã∑Ë¥ùÂÆåÊï¥ÔºåÂê¶Âàô‰ºöÂæóÂà∞401

* Áü≠ÊúütokenËé∑Âèñ(Áü≠ÊúütokenÁöÑËé∑Âèñ‰∏ç‰æùËµñ‰∫éÈÖçÁΩÆÊñá‰ª∂SecretÁöÑÂàõÂª∫)

  > kubectl -n kubernetes-dashboard create token admin-user



## 6. ÈõÜÁæ§ÂàùÂßãÂåñËøáÁ®ã‰∏≠ÁöÑÂÖ∂‰ªñÂ∏∏ËßÅÈóÆÈ¢ò

### <font color=red>Â∏∏ËßÅÈóÆÈ¢ò1 Ôºö ÈõÜÁæ§ÂàùÂßãÂåñËøáÁ®ã‰∏≠ÈïúÂÉèÊãâÂèñÂ§±Ë¥•</font>

```
lighthouse@VM-4-11-ubuntu:~$ sudo kubeadm init
[init] Using Kubernetes version: v1.28.4
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W1206 16:25:29.487834 3261384 checks.go:835] detected that the sandbox image "registry.k8s.io/pause:3.8" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR ImagePull]: failed to pull image registry.k8s.io/kube-apiserver:v1.28.4: output: E1206 16:17:51.493681 3267795 remote_image.go:171] "PullImage from image service failed" err="rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \"registry.k8s.io/kube-apiserver:v1.28.4\": failed to resolve reference \"registry.k8s.io/kube-apiserver:v1.28.4\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-apiserver/manifests/v1.28.4\": dial tcp 108.177.125.82:443: i/o timeout" image="registry.k8s.io/kube-apiserver:v1.28.4"
time="2023-12-06T16:17:51+08:00" level=fatal msg="pulling image: rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \"registry.k8s.io/kube-apiserver:v1.28.4\": failed to resolve reference \"registry.k8s.io/kube-apiserver:v1.28.4\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-apiserver/manifests/v1.28.4\": dial tcp 108.177.125.82:443: i/o timeout"
, error: exit status 1
        [ERROR ImagePull]: failed to pull image registry.k8s.io/kube-controller-manager:v1.28.4: output: E1206 16:20:24.253612 3275462 remote_image.go:171] "PullImage from image service failed" err="rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \"registry.k8s.io/kube-controller-manager:v1.28.4\": failed to resolve reference \"registry.k8s.io/kube-controller-manager:v1.28.4\": failed to do request: Head \"https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-controller-manager/manifests/v1.28.4\": dial tcp 142.251.170.82:443: i/o timeout" image="registry.k8s.io/kube-controller-manager:v1.28.4"
```

ÂéüÂõ†Ôºökubeadm init ÂëΩ‰ª§ÈªòËÆ§‰ΩøÁî®ÁöÑ[docker](https://cloud.tencent.com/product/tke?from_column=20065&from=20065)[ÈïúÂÉè‰ªìÂ∫ì](https://cloud.tencent.com/product/tcr?from_column=20065&from=20065)‰∏∫k8s.gcr.ioÔºåÂõΩÂÜÖÊó†Ê≥ïÁõ¥Êé•ËÆøÈóÆÔºå

üëâ Ëß£ÂÜ≥ÊñπÊ°àÔºöÊõ¥Êç¢‰∏∫ÂõΩÂÜÖÂèØ‰ª•ËÆøÈóÆÁöÑÈïúÂÉè‰ªì„ÄÇ

**ÂÜçÊ¨°Â∞ùËØïÊãâÂèñÈïúÂÉèÔºö**

> sudo kubeadm config images pull --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers

```
lighthouse@VM-4-11-ubuntu:~$ sudo kubeadm config images pull --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.28.4
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.28.4
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.28.4
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.28.4
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.9-0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.10.1
```

**ÂàùÂßãÂåñÊó∂Â¢ûÂä†ÈïúÂÉè‰ªìÂ∫ìÈÖçÁΩÆÊåáÂêëÈòøÈáå‰∫ë**

> sudo kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers --cri-socket unix:///var/run/cri-dockerd.sock --v=5 

### <font color=red>Â∏∏ËßÅÈóÆÈ¢ò2 Ôºö ÂêØÂä®control plane Ë∂ÖÊó∂</font> 

```
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
        timed out waiting for the condition

This error is likely caused by:
        - The kubelet is not running
        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
        - 'systemctl status kubelet'
        - 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
        - 'crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a | grep kube | grep -v pause'
        Once you have found the failing container, you can inspect its logs with:
        - 'crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock logs CONTAINERID'
error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher
```

Ê†πÊçÆÊó•ÂøóÊèêÁ§∫ËøõË°åÂàÜÊûêÔºåÂú®‰∏™‰∫∫ËøõË°åÂàùÂßãÂåñÈõÜÁæ§Êó∂ÈÄöËøáÂÆâË£ÖdockerÁöÑCRIËøõË°åËß£ÂÜ≥„ÄÇ

## 7. ÈõÜÁæ§ÈáçÁΩÆ

Âú®ÊµãËØïËøáÊàêÂèØËÉΩÈúÄË¶ÅÂ§öÊ¨°Â∞ùËØïÔºåÈúÄË¶ÅÁî®Âà∞`kubeadm reset `Êåá‰ª§ËøõË°åÈõÜÁæ§ÈáçÁΩÆ„ÄÇÂ≠òÂú®Â§ö‰∏™ÂÆπÂô®Êó∂Ë¶ÅÈÄöËøá --cri-socketÊåáÂÆöÂÆπÂô®ÔºåÂ¶Ç`sudo kubeadm reset --cri-socket unix:///var/run/cri-dockerd.sock`

ÂÖ≥ËÅîÊñá‰ª∂Ê∏ÖÁêÜ/ÂàùÂßãÂåñ

> sudo rm -rf /etc/cni/net.d

> sudo rm $HOME/.kube/config

